# Interactive Evaluation Viewer Guide

## Overview

The **`evaluation-viewer.html`** is a standalone, serverless HTML page that lets you view LLM evaluation reports directly in your browser without any Python scripts or server setup.

## ðŸš€ Quick Start

### Simply Open & Load

1. **Open the file** in any modern browser:
   ```bash
   # Windows
   start evaluation-viewer.html
   
   # macOS
   open evaluation-viewer.html
   
   # Linux
   xdg-open evaluation-viewer.html
   ```

2. **Load your evaluation data** by either:
   - **Drag & drop** a JSON file onto the drop zone
   - **Click** the drop zone to browse and select a file

3. **Explore results** - The page will instantly display:
   - Model performance rankings
   - Detailed question-by-question analysis
   - Interactive score breakdowns

## Features

### âœ… Completely Serverless
- No Python installation required
- No web server needed
- Works 100% offline
- Pure HTML + CSS + JavaScript

### âœ… Drag & Drop Interface
- Simple file loading
- Visual feedback on drop
- Instant processing

### âœ… Interactive Analysis
- Click any score to see details
- Compare Gemini's ideal answer vs model response
- View justifications for each score

### âœ… Performance Summary
- Model rankings by average score
- Color-coded performance indicators
- Total questions answered

### âœ… Responsive Design
- Works on desktop, tablet, and mobile
- Clean, modern UI with Tailwind CSS
- Smooth animations and transitions

## Compatible Files

The viewer works with any JSON evaluation report generated by:

### Generated Files
- `gemini_evaluation_report_YYYY-MM-DD_HH-MM-SS.json` (timestamped)
- `gemini_evaluation_report.json` (legacy)
- Any custom evaluation JSON with the same structure

### Location Examples
```
Crisis-AI-evaluation/
â”œâ”€â”€ gemini_evaluation_report_2025-10-09_14-24-25.json  âœ…
â”œâ”€â”€ gemini_evaluation_report.json                       âœ…
â”œâ”€â”€ test_results/
â”‚   â””â”€â”€ 2025-10-09_1/
â”‚       â””â”€â”€ gemini_evaluation_report_*.json            âœ…
```

## How It Works

### 1. File Loading
```javascript
// User drags or selects JSON file
handleFile(file) {
    const reader = new FileReader();
    reader.onload = (e) => {
        const data = JSON.parse(e.target.result);
        processReportData(data);
    };
    reader.readAsText(file);
}
```

### 2. Data Processing
```javascript
// Extract model scores
for (const category of Object.values(data)) {
    for (const entry of subcategory) {
        // Calculate averages
        // Generate rankings
        // Build UI
    }
}
```

### 3. Dynamic UI Generation
```javascript
// Generate summary cards
generateSummaryCards(data);

// Generate detailed tables
generateDetailedReport(data);

// Enable interactive toggles
toggleDetails(detailsId);
```

## UI Components

### Performance Summary Cards

Each card shows:
- **Model Name** - Clean, readable name
- **Rank** - Position among all models
- **Average Score** - Mean score across all questions
- **Question Count** - Total questions answered

**Color Coding:**
- ðŸŸ¢ Green (8-10): Excellent performance
- ðŸŸ¡ Yellow (5-7): Good performance
- ðŸ”´ Red (0-4): Needs improvement

### Detailed Evaluation Tables

For each category and subcategory:
- **Question Text** - The crisis scenario question
- **Model Scores** - Clickable score badges
- **Details Panel** - Expands to show:
  - Gemini's ideal answer
  - Model's actual answer
  - Score justification

### Interactive Features

**Click any score** to reveal:
1. **Side-by-side comparison** - Ideal vs actual answer
2. **Justification** - Why the model received this score
3. **Context** - Full question and responses

**Keyboard Navigation:**
- Click anywhere to close open panels
- ESC key support (future enhancement)

## Browser Compatibility

### âœ… Fully Supported
- Chrome 90+
- Firefox 88+
- Edge 90+
- Safari 14+

### Features Used
- **FileReader API** - Read local files
- **JSON.parse** - Parse evaluation data
- **ES6+ JavaScript** - Modern syntax
- **Tailwind CSS** - Via CDN
- **CSS Grid & Flexbox** - Responsive layout

## Security & Privacy

### ðŸ”’ Your Data Stays Local

- **No uploads** - Files never leave your computer
- **No tracking** - No analytics or external calls
- **No server** - Runs entirely in browser
- **No storage** - Data only in memory

### File Access
The viewer only accesses files you explicitly:
- Drag onto the drop zone
- Select via file browser

## Troubleshooting

### "Error processing data"

**Possible Causes:**
- Invalid JSON format
- Corrupted file
- Wrong file structure

**Solution:**
1. Verify JSON is valid: Use online validator or `python -m json.tool file.json`
2. Ensure file generated by `test-evaluation.py`
3. Check file isn't empty or truncated

### "Error reading file"

**Possible Causes:**
- File permissions
- File locked by another program
- File too large (>100MB)

**Solution:**
1. Close other programs using the file
2. Check file permissions
3. Try a smaller evaluation file

### Styles Not Loading

**Possible Causes:**
- No internet connection (Tailwind CSS via CDN)
- Browser blocking CDN

**Solution:**
1. Check internet connection
2. Allow CDN in browser settings
3. Alternatively, download Tailwind CSS for offline use

### Blank Page After Loading

**Possible Causes:**
- Empty evaluation file
- No model evaluations in data
- All models filtered out (Mistral-7B)

**Solution:**
1. Check evaluation file has data
2. Ensure models were evaluated
3. Look for console errors (F12 Developer Tools)

## Advanced Usage

### Loading Multiple Reports

Compare different evaluation runs:

1. Open viewer in multiple browser tabs
2. Load different JSON files in each tab
3. Compare results side-by-side

### Saving Results

To save a particular view:

1. **Browser Bookmark** - Save page state (limited)
2. **Screenshot** - Capture summary or details
3. **Print to PDF** - Save full report (Ctrl+P)

### Customization

The viewer is a single HTML file - you can customize:

#### Colors
```javascript
// Change score thresholds
function getScoreColor(score) {
    if (score >= 9) return "bg-green-100 text-green-800";  // Stricter
    if (score >= 6) return "bg-yellow-100 text-yellow-800";
    return "bg-red-100 text-red-800";
}
```

#### Layout
```html
<!-- Change grid columns -->
<div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 xl:grid-cols-5">
    <!-- More columns = more cards per row -->
</div>
```

#### Filtering
```javascript
// Show/hide specific models
if (modelName && 
    !modelName.toLowerCase().includes('mistral-7b') &&
    !modelName.toLowerCase().includes('unwanted-model')) {
    modelNames.add(modelName);
}
```

## Comparison: Viewer vs Generator

| Feature | evaluation-viewer.html | html-report-generator.py |
|---------|----------------------|--------------------------|
| **Setup** | None - just open | Requires Python |
| **File Loading** | Drag & drop any file | Command line argument |
| **Updates** | Load new file anytime | Re-run script |
| **Portability** | Single HTML file | Script + output file |
| **Internet** | CDN for CSS (optional) | None needed |
| **Best For** | Interactive exploration | Embedded reports |

## Tips & Best Practices

### ðŸŽ¯ Efficient Workflow

1. **Keep viewer bookmarked** - Quick access
2. **Use latest evaluation files** - Most recent results
3. **Compare batch folders** - Load different batches to track progress
4. **Screenshot highlights** - Save interesting comparisons

### ðŸ“Š Analysis Workflow

1. **Start with summary** - Identify top/bottom performers
2. **Drill into details** - Click low scores to understand issues
3. **Compare answers** - See where models diverge
4. **Track patterns** - Notice common failure modes

### ðŸ”„ Iterative Testing

1. Run batch tests â†’ Generate evaluation
2. Load in viewer â†’ Identify weaknesses  
3. Adjust prompts/models â†’ Re-test
4. Compare new results â†’ Track improvements

## Example Use Cases

### Use Case 1: Quick Model Comparison

```
Goal: Which model performed best overall?

Steps:
1. Open evaluation-viewer.html
2. Drag gemini_evaluation_report_latest.json
3. Check summary cards - sorted by score
4. Top card = best model

Result: Instant ranking in 10 seconds
```

### Use Case 2: Debugging Poor Performance

```
Goal: Why did Model X score low on safety questions?

Steps:
1. Load evaluation file
2. Scroll to "Safety" category
3. Click red scores (0-4)
4. Read justifications
5. Compare to Gemini's ideal answer

Result: Understand specific failure points
```

### Use Case 3: Batch Comparison

```
Goal: Did my model improve after fine-tuning?

Steps:
1. Open viewer in Tab 1 â†’ Load before.json
2. Open viewer in Tab 2 â†’ Load after.json
3. Compare summary scores
4. Check specific categories

Result: Data-driven improvement validation
```

## Summary

### âœ… Advantages

- **Zero setup** - Open and use immediately
- **Offline capable** - No server or internet needed
- **Interactive** - Click to explore details
- **Portable** - Single file, works anywhere
- **Privacy-first** - Data never leaves your machine

### ðŸŽ¯ When to Use

- Quick results viewing
- Interactive exploration
- Comparing multiple evaluations
- Sharing reports (send HTML + JSON)
- Mobile/tablet viewing

### ðŸ“š Related Tools

- `batch_test_models.py` - Generate test results
- `eval_batch.py` - Evaluate batches
- `test-evaluation.py` - Create JSON reports
- `html-report-generator.py` - Generate static HTML

---

**Ready to explore your results?**

Just open `evaluation-viewer.html` and drag your JSON file! ðŸš€

